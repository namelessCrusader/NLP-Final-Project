{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKjSVK0h8_5U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import random\n",
        "import regex as re\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import gensim.downloader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split,KFold\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhbHHKS0JU6h"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive\"\n",
        "# Should we load it from kaggle each time?\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOcsAnkd8_5Y"
      },
      "outputs": [],
      "source": [
        "def build_vocab_from_docs(list_of_documents):\n",
        "    temp = []\n",
        "    for document in list_of_documents:\n",
        "        temp.extend(document)\n",
        "        temp = list(set(temp))\n",
        "    vocabulary = []\n",
        "    return vocabulary\n",
        "\n",
        "def word_frequency_dictionary(list_of_documents):\n",
        "    frequency_dictionary = {}\n",
        "    for document in list_of_documents:\n",
        "        for word in document.split(\" \"):\n",
        "            try:\n",
        "                frequency_dictionary[word] += 1\n",
        "            except:\n",
        "                frequency_dictionary[word] = 1\n",
        "    return frequency_dictionary\n",
        "\n",
        "def build_vocab_from_dictionary_atleast_N(frequency_dictionary, N):\n",
        "    temp = [word for word, count in frequency_dictionary.items() if count > N]\n",
        "    vocabulary = []\n",
        "    for word in temp:\n",
        "        if word not in stopwords.words('english'):\n",
        "            vocabulary.append(word)\n",
        "\n",
        "    return vocabulary\n",
        "\n",
        "def build_vocab_from_dictionary_top_N(frequency_dictionary, N):\n",
        "\n",
        "    # Use sorted with a custom key to sort by counts in descending order\n",
        "    sorted_words = sorted(frequency_dictionary.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the top N words from the sorted list\n",
        "    vocabulary = [word for word, count in sorted_words[:N]]\n",
        "    return vocabulary\n",
        "\n",
        "def remove_stop_words(documents):\n",
        "    clean_documents = []\n",
        "    current_document = []\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    for document in documents:\n",
        "        current_document = [word for word in document if word not in stop_words]\n",
        "        clean_documents.append(current_document)\n",
        "    return clean_documents\n",
        "\n",
        "def clean(sentance):\n",
        "    '''\n",
        "    Input:\n",
        "        review: a string containing a review.\n",
        "    Output:\n",
        "        review_cleaned: a processed review.\n",
        "    '''\n",
        "    sentance_lower = sentance.lower()\n",
        "\n",
        "    sentance_links_removed = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sentance_lower, flags=re.MULTILINE)\n",
        "\n",
        "    sentance_stopwords_removed = ' '.join([word for word in sentance_links_removed.split() if word not in (stopwords.words('english'))])\n",
        "\n",
        "    regex = re.compile('<([^>]+)>')\n",
        "    sentance_escape = regex.sub(\"\",sentance_stopwords_removed)\n",
        "\n",
        "    modified_string = ''.join([i for i in sentance_escape if not i.isdigit()])\n",
        "\n",
        "    sentance_punctuation_space = modified_string.replace(\"-\",\" \")\n",
        "    sentance_punctuation_space = sentance_punctuation_space.replace(\"_\",\" \")\n",
        "    sentance_punctuation_space = sentance_punctuation_space.replace(\".\",\" \")\n",
        "\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    sentance_punctuation_remove = regex.sub('', sentance_punctuation_space)\n",
        "\n",
        "    #porter = nltk.stem.PorterStemmer()\n",
        "    #sentance_cleaned = \" \".join([porter.stem(word = word) for word in sentance_punctuation_remove.split(\" \")])\n",
        "\n",
        "    return sentance_punctuation_remove\n",
        "\n",
        "def remove_nonvocab_words(documents):\n",
        "    clean_documents = []\n",
        "    current_document = []\n",
        "\n",
        "    vocab = set(vocabulary)\n",
        "    for document in documents:\n",
        "        current_document = [word for word in document.split() if word in vocab]\n",
        "        clean_documents.append(\" \".join(current_document))\n",
        "    return clean_documents\n",
        "\n",
        "def encode_and_pad(doc, length):\n",
        "    sos = [word2index[\"<SOS>\"]]\n",
        "    eos = [word2index[\"<EOS>\"]]\n",
        "    pad = [word2index[\"<PAD>\"]]\n",
        "\n",
        "    if len(doc) < length - 2: # -2 for SOS and EOS\n",
        "        n_pads = length - 2 - len(doc)\n",
        "        encoded = [word2index[w] for w in doc]\n",
        "        return sos + encoded + eos + pad * n_pads\n",
        "    else: # tweet is longer than possible; truncating\n",
        "        encoded = [word2index[w] for w in doc]\n",
        "        truncated = encoded[:length - 2]\n",
        "        return sos + truncated + eos\n",
        "def convert_to_one_hot(lst):\n",
        "    # Find the index of the maximum element\n",
        "    max_index = max(range(len(lst)), key=lst.__getitem__)\n",
        "\n",
        "    # Create a new list with all zeros\n",
        "    one_hot = [0] * len(lst)\n",
        "\n",
        "    # Set the maximum element's index to 1\n",
        "    one_hot[max_index] = 1\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "def labels_to_one_hot(labels, lowest, highest):\n",
        "    \"\"\"\n",
        "    Convert labels to one-hot encoded labels based on the given lowest and highest values.\n",
        "\n",
        "    Parameters:\n",
        "    - labels (numpy array or list): The input labels.\n",
        "    - lowest (int): The lowest value in the labels.\n",
        "    - highest (int): The highest value in the labels.\n",
        "\n",
        "    Returns:\n",
        "    - one_hot_labels (numpy array): The one-hot encoded labels.\n",
        "    \"\"\"\n",
        "    num_classes = highest - lowest + 1\n",
        "    one_hot_labels = np.eye(num_classes)[np.array(labels) - lowest]\n",
        "    return one_hot_labels\n",
        "\n",
        "def train(model,train_x,train_y):\n",
        "    epochs = 50\n",
        "    train_losses = []\n",
        "    val_accuracy = []\n",
        "\n",
        "    h0, c0 = model.init_hidden(batch_size)\n",
        "\n",
        "    h0 = h0.to(device)\n",
        "    c0 = c0.to(device)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_x,train_y)):\n",
        "\n",
        "        train_data, val_data = train_x[train_idx], train_x[val_idx]\n",
        "        train_target, val_target = train_y[train_idx], train_y[val_idx]\n",
        "\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        train_data_tensor = torch.from_numpy(train_data)\n",
        "        val_data_tensor = torch.from_numpy(val_data)\n",
        "        train_target_tensor = torch.from_numpy(train_target)\n",
        "        val_target_tensor = torch.from_numpy(val_target)\n",
        "\n",
        "\n",
        "        # Create TensorDatasets with Long indices\n",
        "        train_ds = TensorDataset(train_data_tensor.long(), train_target_tensor)\n",
        "        val_ds = TensorDataset(val_data_tensor.long(), val_target_tensor)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_dl = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "        val_dl = DataLoader(val_ds, shuffle=False, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "        for e in range(epochs):\n",
        "\n",
        "            for batch_idx, batch in enumerate(train_dl):\n",
        "\n",
        "                input = batch[0].to(device)\n",
        "                target = batch[1].float().to(device)  # Convert target to float\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    out = model(input, (h0, c0))\n",
        "                    loss = criterion(out, target)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            batch_acc = []\n",
        "\n",
        "            for batch_idx, batch in enumerate(val_dl):\n",
        "                input = batch[0].to(device)\n",
        "                target = batch[1].to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(False):\n",
        "                    out = model(input, (h0, c0))\n",
        "                    _, preds = torch.max(out, 1)\n",
        "                    preds = preds.to(\"cpu\").tolist()\n",
        "                    target = target.to(\"cpu\").tolist()\n",
        "                    preds = labels_to_one_hot(preds, 0, 2)\n",
        "\n",
        "                    batch_acc.append(accuracy_score(preds, target))\n",
        "            val_acc = sum(batch_acc) / len(batch_acc)\n",
        "            train_losses.append(loss.item())\n",
        "            val_accuracy.append(val_acc)\n",
        "    return train_losses, val_accuracy, model\n",
        "\n",
        "def test(model,test_dl):\n",
        "\n",
        "    batch_acc = []\n",
        "    h0, c0 = model.init_hidden(batch_size)\n",
        "\n",
        "    h0 = h0.to(device)\n",
        "    c0 = c0.to(device)\n",
        "\n",
        "\n",
        "    for batch_idx, batch in enumerate(test_dl):\n",
        "        input = batch[0].to(device)\n",
        "        target = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(False):\n",
        "            out = model(input, (h0, c0))\n",
        "            _, preds = torch.max(out, 1)\n",
        "            preds = preds.to(\"cpu\").tolist()\n",
        "            target = target.to(\"cpu\").tolist()\n",
        "            preds = labels_to_one_hot(preds, 0, 2)\n",
        "\n",
        "            batch_acc.append(accuracy_score(preds, target))\n",
        "\n",
        "    return sum(batch_acc) / len(batch_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2oLmtz7L49v"
      },
      "source": [
        "#Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTMUFRP88_5Z"
      },
      "outputs": [],
      "source": [
        "if \"dataset_clean.csv\" not in os.listdir(path):\n",
        "    dataset = pd.read_csv(path+'/data.csv')\n",
        "    dataset_X = dataset['Sentence']\n",
        "    dataset_Y = dataset['Sentiment']\n",
        "    Y_hot_encoded = []\n",
        "    for i in dataset_Y:\n",
        "      if i == 'positive':\n",
        "        Y_hot_encoded.append([0,0,1])\n",
        "      if i == 'neutral':\n",
        "        Y_hot_encoded.append([0,1,0])\n",
        "      if i == 'negative':\n",
        "        Y_hot_encoded.append([1,0,0])\n",
        "    X_cleaned = [clean(data_point) for data_point in dataset_X]\n",
        "    frequency_dictionary = word_frequency_dictionary(X_cleaned)\n",
        "    vocabulary = build_vocab_from_dictionary_top_N(frequency_dictionary, 10000)\n",
        "    vocabulary = sorted(vocabulary)\n",
        "    vocabulary = list(filter(lambda el: len(el) > 2, vocabulary))\n",
        "    X_cleaned = remove_nonvocab_words(X_cleaned)\n",
        "    dataset_clean = pd.DataFrame({\"Data\":X_cleaned,\"Labels\":Y_hot_encoded})\n",
        "    dataset_clean['Data'] = remove_nonvocab_words(dataset_clean['Data'])\n",
        "    dataset_clean.to_csv(path+ \"/dataset_clean.csv\")\n",
        "    with open(path+\"/vocabulary.txt\",\"w+\") as file:\n",
        "      for word in vocabulary:\n",
        "        file.write(word)\n",
        "        file.write(\"\\n\")\n",
        "\n",
        "else:\n",
        "    dataset_clean = pd.read_csv(path+'/dataset_clean.csv').dropna()\n",
        "    dataset_clean['Labels'] = dataset_clean['Labels'].apply(ast.literal_eval)\n",
        "    dataset_clean = dataset_clean.drop(dataset_clean.columns[0],axis=1)\n",
        "    with open(path+\"/vocabulary.txt\",\"r\") as file:\n",
        "      vocabulary = file.read().split(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bsP--kD6b9S"
      },
      "source": [
        "# Dataset Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gn6-ornbVvq"
      },
      "source": [
        "# LSTM\n",
        "\n",
        "Consider adding a CNN layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAIbxRp7WO8E"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Tokenize the text data\n",
        "max_len = 36  # Largest sentence is 36 tokens long\n",
        "tokenizer = lambda x: x.split()[:max_len]  # Use a tokenizer that limits the length\n",
        "dataset_clean['Data'] = dataset_clean['Data'].apply(tokenizer)\n",
        "\n",
        "\n",
        "train_clean_df, test_clean_df = train_test_split(dataset_clean, test_size=0.2)\n",
        "\n",
        "train_clean_df[:3]\n",
        "\n",
        "index2word = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
        "index2word.extend(vocabulary)\n",
        "word2index = {token: idx for idx, token in enumerate(index2word)}\n",
        "\n",
        "train_encoded = [(encode_and_pad(row['Data'], max_len), row['Labels']) for index, row in train_clean_df.iterrows()]\n",
        "test_encoded = [(encode_and_pad(row['Data'], max_len), row['Labels']) for index, row in test_clean_df.iterrows()]\n",
        "\n",
        "batch_size = 20\n",
        "\n",
        "train_x = np.array([doc for doc, label in train_encoded])\n",
        "train_y = np.array([label for doc, label in train_encoded])\n",
        "\n",
        "test_x = np.array([doc for doc, label in test_encoded])\n",
        "test_y = np.array([label for doc, label in test_encoded])\n",
        "\n",
        "test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "test_dl = DataLoader(test_ds, shuffle=True, batch_size=batch_size, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU23z5fb0Tda"
      },
      "outputs": [],
      "source": [
        "LSTM_path = path + \"/LSTM_Model_Weights_History\"\n",
        "if not os.path.exists(LSTM_path):\n",
        "    # Folder doesn't exist, so create it\n",
        "    os.makedirs(LSTM_path)\n",
        "print(os.listdir(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWtusHd1S_Hh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, scale_factor, attn_dropout=0.1):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale_factor\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        output = torch.matmul(attn_weights, value)\n",
        "\n",
        "        return output, attn_weights\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BiLSTM_SentimentAnalysis(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, bidirectional=False, attention=False):\n",
        "        super(BiLSTM_SentimentAnalysis, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
        "        self.attention = attention\n",
        "        if self.attention :\n",
        "            self.attn = ScaledDotProductAttention(scale_factor=(hidden_dim * 2) ** 0.5 if bidirectional else (hidden_dim) ** 0.5)\n",
        "            self.attn_dropout = nn.Dropout(dropout)\n",
        "            self.lstm_k = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
        "            self.lstm_v = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
        "            self.lstm_q = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        if self.attention:\n",
        "            lstm_out_k, (hidden_k, cell_k) = self.lstm_k(embedded)\n",
        "            lstm_out_v, (hidden_v, cell_v) = self.lstm_v(embedded)\n",
        "            lstm_out_q, (hidden_q, cell_q) = self.lstm_q(embedded)\n",
        "\n",
        "            lstm_out_q = lstm_out_q.permute(1, 0, 2)\n",
        "            lstm_out_k = lstm_out_k.permute(1, 0, 2)\n",
        "            lstm_out_v = lstm_out_v.permute(1, 0, 2)\n",
        "            attn_output, _ = self.attn(lstm_out_q, lstm_out_k, lstm_out_v)\n",
        "            attn_output = self.attn_dropout(attn_output)\n",
        "            lstm_out = attn_output.permute(1, 0, 2)\n",
        "            # Aggregate hidden and cell, max pooling/average pooling or averaging or weighted sum(could learn weighinh)\n",
        "            #Maxpooling rn\n",
        "            hidden, _ = torch.max(torch.stack([hidden_k, hidden_v, hidden_q]), dim=0)\n",
        "            cell, _ = torch.max(torch.stack([cell_k, cell_v, cell_q]), dim=0)\n",
        "\n",
        "        else:\n",
        "            lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        hidden = hidden[-1, :, :] if not self.lstm.bidirectional else torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        hidden = self.dropout(hidden)\n",
        "\n",
        "        return self.fc(hidden)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        num_directions = 2 if self.lstm.bidirectional else 1\n",
        "        return (torch.zeros(num_directions, batch_size, hidden_dim),\n",
        "                torch.zeros(num_directions, batch_size, hidden_dim))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi0JPvm1LaMM"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "hidden_dims = [16,32,64,128,256,512]\n",
        "output_dim = 3\n",
        "dropouts = [0.0,0.25,0.5]\n",
        "vocab_size = len(word2index)\n",
        "num_epochs = 3\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEctQ8hHCSUo"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = {}\n",
        "model_configs = {\"BiLSTM_History_WO_Attention\":[True,False],\"LSTM_History_W_Scaled_Dot_Attention\":[False,True],\"BiLSTM_History_W_Scaled_Dot_Attention\":[True,True]}\n",
        "\n",
        "for model_config_name,model_config in model_configs.items():\n",
        "  print(model_config_name)\n",
        "  for hidden_dim in hidden_dims:\n",
        "    for dropout in dropouts:\n",
        "      model = BiLSTM_SentimentAnalysis(vocab_size, embedding_dim, hidden_dim, output_dim, dropout, bidirectional=model_config[0],attention = model_config[1])\n",
        "      model = model.to(device)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "\n",
        "      train_losses, val_accuracy, model = train(model,train_x,train_y)\n",
        "      model_name = model_config_name +\"hidden_dim_\"+str(hidden_dim)+\"_dropout\"+str(dropout)\n",
        "      history[model_name] = (train_losses, val_accuracy, test(model,test_dl))\n",
        "      print(\"_________________________________________________________________________\")\n",
        "\n",
        "      print(model_name,\" Test Accuracy: \",test(model,test_dl))\n",
        "\n",
        "      plt.plot(train_losses)\n",
        "      plt.title(\"Training Losses\")\n",
        "      plt.show()\n",
        "      plt.plot(val_accuracy)\n",
        "      plt.title(\"Validation Accuracy\")\n",
        "      plt.show()\n",
        "\n",
        "      print(\"_________________________________________________________________________\")\n",
        "      torch.save(model.state_dict(),LSTM_path + \"/\" + model_name +\".pt\")\n",
        "\n",
        "  with open(LSTM_path + \"/\" + model_config_name + '.pkl', 'wb') as file:\n",
        "    pickle.dump(history, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV8X7-abAFbD"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = {}\n",
        "model_config_name = \"LSTM_History_W_Scaled_Dot_Attention\"\n",
        "model_config = [False,True]\n",
        "\n",
        "print(model_config_name)\n",
        "for hidden_dim in hidden_dims:\n",
        "    for dropout in dropouts:\n",
        "        model = BiLSTM_SentimentAnalysis(vocab_size, embedding_dim, hidden_dim, output_dim, dropout, bidirectional=model_config[0],attention = model_config[1])\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "\n",
        "        train_losses, val_accuracy, model = train(model,train_x,train_y)\n",
        "        model_name = model_config_name +\"hidden_dim_\"+str(hidden_dim)+\"_dropout\"+str(dropout)\n",
        "        history[model_name] = (train_losses, val_accuracy, test(model,test_dl))\n",
        "        print(\"_________________________________________________________________________\")\n",
        "\n",
        "        print(model_name,\" Test Accuracy: \",test(model,test_dl))\n",
        "\n",
        "        plt.plot(train_losses)\n",
        "        plt.title(\"Training Losses\")\n",
        "        plt.show()\n",
        "        plt.plot(val_accuracy)\n",
        "        plt.title(\"Validation Accuracy\")\n",
        "        plt.show()\n",
        "\n",
        "        print(\"_________________________________________________________________________\")\n",
        "        torch.save(model.state_dict(),LSTM_path + \"/\" + model_name +\".pt\")\n",
        "\n",
        "with open(LSTM_path + \"/\" + model_config_name + '.pkl', 'wb') as file:\n",
        "    pickle.dump(history, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s-QVsc3f5EM1"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = {}\n",
        "model_config_name = \"LSTM_History_WO_Attention\"\n",
        "model_config = [False,False]\n",
        "\n",
        "print(model_config_name)\n",
        "for hidden_dim in hidden_dims:\n",
        "    for dropout in dropouts:\n",
        "        model = BiLSTM_SentimentAnalysis(vocab_size, embedding_dim, hidden_dim, output_dim, dropout, bidirectional=model_config[0],attention = model_config[1])\n",
        "        model = model.to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "\n",
        "        train_losses, val_accuracy, model = train(model,train_x,train_y)\n",
        "        model_name = model_config_name +\"hidden_dim_\"+str(hidden_dim)+\"_dropout\"+str(dropout)\n",
        "        history[model_name] = (train_losses, val_accuracy, test(model,test_dl))\n",
        "        print(\"_________________________________________________________________________\")\n",
        "\n",
        "        print(model_name,\" Test Accuracy: \",test(model,test_dl))\n",
        "\n",
        "        plt.plot(train_losses)\n",
        "        plt.title(\"Training Losses\")\n",
        "        plt.show()\n",
        "        plt.plot(val_accuracy)\n",
        "        plt.title(\"Validation Accuracy\")\n",
        "        plt.show()\n",
        "\n",
        "        print(\"_________________________________________________________________________\")\n",
        "        torch.save(model.state_dict(),LSTM_path + \"/\" + model_name +\".pt\")\n",
        "\n",
        "with open(LSTM_path + \"/\" + model_config_name + '.pkl', 'wb') as file:\n",
        "    pickle.dump(history, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khTaEs9xjQK2"
      },
      "outputs": [],
      "source": [
        "# Pick the ideal parameters based on the previous set of tests and use them to train the different types of models on multiple epochs\n",
        "# NOT DONE YET, RAN BY MISTAKE, DISREGARD TILL THE HYPERPARAMS ARE SET TO ONE VALUE\n",
        "embedding_dim = 100\n",
        "hidden_dims = [16,32,64,128,256,512]\n",
        "output_dim = 3\n",
        "dropouts = [0.0,0.25,0.5]\n",
        "vocab_size = len(word2index)\n",
        "num_epochs = 3\n",
        "# Initialize KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "history = {}\n",
        "model_configs = {\"BiLSTM_History_WO_Attention_3_epochs\":[True,False],\"LSTM_History_W_Scaled_Dot_Attention_3_epochs\":[False,True],\"BiLSTM_History_W_Scaled_Dot_Attention_3_epochs\":[True,True]}\n",
        "\n",
        "for model_config_name,model_config in model_configs.items():\n",
        "  for epoch in range(num_epochs):\n",
        "      print(model_config_name)\n",
        "      model = BiLSTM_SentimentAnalysis(vocab_size, embedding_dim, hidden_dim, output_dim, dropout, bidirectional=model_config[0],attention = model_config[1])\n",
        "      model = model.to(device)\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
        "\n",
        "      train_losses, val_accuracy, model = train(model,train_x,train_y)\n",
        "      model_name = model_config_name+\"_\" + str(epoch) +\"_epoch\"\n",
        "      history[model_name] = (train_losses, val_accuracy, test(model,test_dl))\n",
        "      print(\"_________________________________________________________________________\")\n",
        "\n",
        "      print(model_name,\" Test Accuracy: \",test(model,test_dl))\n",
        "\n",
        "      plt.plot(train_losses)\n",
        "      plt.title(\"Training Losses\")\n",
        "      plt.show()\n",
        "      plt.plot(val_accuracy)\n",
        "      plt.title(\"Validation Accuracy\")\n",
        "      plt.show()\n",
        "\n",
        "      print(\"_________________________________________________________________________\")\n",
        "      torch.save(model.state_dict(),LSTM_path + \"/\" + model_name +\".pt\")\n",
        "\n",
        "  with open(LSTM_path + \"/\" + model_config_name + '.pkl', 'wb') as file:\n",
        "    pickle.dump(history, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E20F9A7ZVQMa"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saiJjEV7sAMy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9rUw2IBs6q3"
      },
      "outputs": [],
      "source": [
        "# Read dataset into a DataFrame\n",
        "df = pd.read_csv('dataset_clean.csv')\n",
        "\n",
        "df['Labels'] = df['Labels'].apply(lambda x: eval(x)).apply(lambda x: x.index(1))\n",
        "\n",
        "# Split the data into a training set and a hold-out test set\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oc_YpEXs8KV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm7hqv61s9Il"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6HJUlRAs-IE"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYrLkWAOs_Hm"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW  # Use the PyTorch AdamW optimizer\n",
        "\n",
        "# Define the training process\n",
        "def train_model(model, train_loader, val_loader, device, optimizer, scheduler, num_epochs=3):\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_acc = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Convert logits to predictions\n",
        "                preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                val_acc += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_val_acc = val_acc / len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss}, Validation Accuracy: {avg_val_acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2ROOmlgtALa"
      },
      "outputs": [],
      "source": [
        "# Initialize KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "num_epochs = 1\n",
        "\n",
        "# Run the K-Fold cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # Create dataset for the current fold\n",
        "    train_dataset = CustomDataset(\n",
        "        texts=df.iloc[train_idx]['Data'].to_numpy(),\n",
        "        labels=df.iloc[train_idx]['Labels'].tolist(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=128\n",
        "    )\n",
        "\n",
        "    val_dataset = CustomDataset(\n",
        "        texts=df.iloc[val_idx]['Data'].to_numpy(),\n",
        "        labels=df.iloc[val_idx]['Labels'].tolist(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=128\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders for the current fold\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # Initialize the model for the current fold\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=3,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    # Train the model on the current fold\n",
        "    train_model(model, train_loader, val_loader, device, optimizer, scheduler, num_epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKLo2hY_tQkF"
      },
      "source": [
        "FOLD 0\n",
        "--------------------------------\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3, Train Loss: 0.6744642976811315\n",
        "Epoch 1/3, Validation Loss: 0.5109011908439366, Validation Accuracy: 0.7660472972972973\n",
        "Epoch 2/3, Train Loss: 0.4453526675167141\n",
        "Epoch 2/3, Validation Loss: 0.5109011908439366, Validation Accuracy: 0.7660472972972973\n",
        "Epoch 3/3, Train Loss: 0.45124847894540826\n",
        "Epoch 3/3, Validation Loss: 0.5109011908439366, Validation Accuracy: 0.7660472972972973\n",
        "FOLD 1\n",
        "--------------------------------\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3, Train Loss: 0.7722194909337438\n",
        "Epoch 1/3, Validation Loss: 0.5854998283289574, Validation Accuracy: 0.7305743243243243\n",
        "Epoch 2/3, Train Loss: 0.5113978568502661\n",
        "Epoch 2/3, Validation Loss: 0.5854998283289574, Validation Accuracy: 0.7305743243243243\n",
        "Epoch 3/3, Train Loss: 0.5123804962268869\n",
        "Epoch 3/3, Validation Loss: 0.5854998283289574, Validation Accuracy: 0.7305743243243243\n",
        "FOLD 2\n",
        "--------------------------------\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3, Train Loss: 0.7310387798340248\n",
        "Epoch 1/3, Validation Loss: 0.5351066613850528, Validation Accuracy: 0.7671232876712328\n",
        "Epoch 2/3, Train Loss: 0.4782958377038253\n",
        "Epoch 2/3, Validation Loss: 0.5351066613850528, Validation Accuracy: 0.7671232876712328\n",
        "Epoch 3/3, Train Loss: 0.474815690781883\n",
        "Epoch 3/3, Validation Loss: 0.5351066613850528, Validation Accuracy: 0.7671232876712328\n",
        "FOLD 3\n",
        "--------------------------------\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3, Train Loss: 0.6929047598722852\n",
        "Epoch 1/3, Validation Loss: 0.5681205738897193, Validation Accuracy: 0.7303082191780822\n",
        "Epoch 2/3, Train Loss: 0.45392714695108627\n",
        "Epoch 2/3, Validation Loss: 0.5681205738897193, Validation Accuracy: 0.7303082191780822\n",
        "Epoch 3/3, Train Loss: 0.461759096373877\n",
        "Epoch 3/3, Validation Loss: 0.5681205738897193, Validation Accuracy: 0.7303082191780822\n",
        "FOLD 4\n",
        "--------------------------------\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3, Train Loss: 0.7161020683347161\n",
        "Epoch 1/3, Validation Loss: 0.5810992050252549, Validation Accuracy: 0.7200342465753424\n",
        "Epoch 2/3, Train Loss: 0.4491735412500824\n",
        "Epoch 2/3, Validation Loss: 0.5810992050252549, Validation Accuracy: 0.7200342465753424\n",
        "Epoch 3/3, Train Loss: 0.44975869113471323\n",
        "Epoch 3/3, Validation Loss: 0.5810992050252549, Validation Accuracy: 0.7200342465753424"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_cSeyfZtROv"
      },
      "outputs": [],
      "source": [
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), 'first_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dss4AekutTwS"
      },
      "outputs": [],
      "source": [
        "test_dataset = CustomDataset(\n",
        "    texts=test_df['Data'].to_numpy(),\n",
        "    labels=test_df['Labels'].tolist(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "# Create the DataLoader for the test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG_yB0hXtUoI"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ittQUZgbtVEG"
      },
      "source": [
        "Test Accuracy: 0.83\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN2h-AdqQ7aZ"
      },
      "source": [
        "# Second BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbANiLDOQ9ZA"
      },
      "outputs": [],
      "source": [
        "# Initialize KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "num_epochs = 3\n",
        "\n",
        "# Run the K-Fold cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # Create dataset for the current fold\n",
        "    train_dataset = CustomDataset(\n",
        "        texts=df.iloc[train_idx]['Data'].to_numpy(),\n",
        "        labels=df.iloc[train_idx]['Labels'].tolist(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=128\n",
        "    )\n",
        "\n",
        "    val_dataset = CustomDataset(\n",
        "        texts=df.iloc[val_idx]['Data'].to_numpy(),\n",
        "        labels=df.iloc[val_idx]['Labels'].tolist(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=128\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders for the current fold\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # Initialize the model for the current fold\n",
        "    model3 = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-large-uncased',\n",
        "        num_labels=3,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False\n",
        "    )\n",
        "    model3.to(device)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = AdamW(model3.parameters(), lr=5e-5)\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    # Train the model on the current fold\n",
        "    train_model(model3, train_loader, val_loader, device, optimizer, scheduler, num_epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GgtV90LQ_Fh"
      },
      "source": [
        "FOLD 0\n",
        "--------------------------------\n",
        "model.safetensors: 100%\n",
        "1.34G/1.34G [00:05<00:00, 150MB/s]\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3, Train Loss: 0.7998394565370709\n",
        "Epoch 1/3, Validation Loss: 0.5376094245427364, Validation Accuracy: 0.7559121621621622\n",
        "Epoch 2/3, Train Loss: 0.5190620001774192\n",
        "Epoch 2/3, Validation Loss: 0.4861597678548581, Validation Accuracy: 0.7820945945945946\n",
        "Epoch 3/3, Train Loss: 0.3650102964745447\n",
        "Epoch 3/3, Validation Loss: 0.4956785092583379, Validation Accuracy: 0.7694256756756757\n",
        "FOLD 1\n",
        "--------------------------------\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3, Train Loss: 0.7596423810049129\n",
        "Epoch 1/3, Validation Loss: 0.5687227275323223, Validation Accuracy: 0.7356418918918919\n",
        "Epoch 2/3, Train Loss: 0.4679378428505956\n",
        "Epoch 2/3, Validation Loss: 0.5206806018344454, Validation Accuracy: 0.7736486486486487\n",
        "Epoch 3/3, Train Loss: 0.30033265167383205\n",
        "Epoch 3/3, Validation Loss: 0.5763120387957709, Validation Accuracy: 0.7677364864864865\n",
        "FOLD 2\n",
        "--------------------------------\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3, Train Loss: 1.0044639240759632\n",
        "Epoch 1/3, Validation Loss: 0.9736260396160491, Validation Accuracy: 0.5522260273972602\n",
        "Epoch 2/3, Train Loss: 1.0000703308362602\n",
        "Epoch 2/3, Validation Loss: 0.9667051214061372, Validation Accuracy: 0.5522260273972602\n",
        "Epoch 3/3, Train Loss: 0.9998794724916842\n",
        "Epoch 3/3, Validation Loss: 0.9657683421487677, Validation Accuracy: 0.5522260273972602\n",
        "FOLD 3\n",
        "--------------------------------\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3, Train Loss: 1.001695082049321\n",
        "Epoch 1/3, Validation Loss: 0.9773599366619162, Validation Accuracy: 0.553082191780822\n",
        "Epoch 2/3, Train Loss: 0.9990754469262862\n",
        "Epoch 2/3, Validation Loss: 0.9743413541414966, Validation Accuracy: 0.553082191780822\n",
        "Epoch 3/3, Train Loss: 0.9943690568513838\n",
        "Epoch 3/3, Validation Loss: 0.9745499067110558, Validation Accuracy: 0.553082191780822\n",
        "FOLD 4\n",
        "--------------------------------\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3, Train Loss: 0.791604362962189\n",
        "Epoch 1/3, Validation Loss: 0.6261102246911558, Validation Accuracy: 0.7106164383561644\n",
        "Epoch 2/3, Train Loss: 0.45570625481116284\n",
        "Epoch 2/3, Validation Loss: 0.5671376418577482, Validation Accuracy: 0.7268835616438356\n",
        "Epoch 3/3, Train Loss: 0.25945606787050135\n",
        "Epoch 3/3, Validation Loss: 0.6489546046885726, Validation Accuracy: 0.7388698630136986"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZtgpheORAfD"
      },
      "outputs": [],
      "source": [
        "test_dataset = CustomDataset(\n",
        "    texts=test_df['Data'].to_numpy(),\n",
        "    labels=test_df['Labels'].tolist(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "# Create the DataLoader for the test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hmEJrJBRDV3"
      },
      "outputs": [],
      "source": [
        "model3.eval()\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model3(input_ids, attention_mask=attention_mask)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khJqTXgoREEK"
      },
      "source": [
        "Test Accuracy: 0.91\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWZfSEAH77qV"
      },
      "source": [
        "# Third BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXoO_SZD79-M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pej4WTG8BYb"
      },
      "outputs": [],
      "source": [
        "# Customize BERT's configuration\n",
        "config = BertConfig(\n",
        "    hidden_size=768,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=16,\n",
        "    num_labels=3,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ML-iLuT8Fdc"
      },
      "outputs": [],
      "source": [
        "# Initialize KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "num_epochs = 1\n",
        "\n",
        "# Run the K-Fold cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # Create dataset for the current fold\n",
        "    train_dataset = CustomDataset(\n",
        "        texts=df.iloc[train_idx]['Data'].to_numpy(),\n",
        "        labels=df.iloc[train_idx]['Labels'].tolist(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=128\n",
        "    )\n",
        "\n",
        "    val_dataset = CustomDataset(\n",
        "        texts=df.iloc[val_idx]['Data'].to_numpy(),\n",
        "        labels=df.iloc[val_idx]['Labels'].tolist(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=128\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders for the current fold\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # Initialize the model for the current fold\n",
        "    model4 = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        config=config\n",
        "    )\n",
        "    model4.to(device)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = AdamW(model4.parameters(), lr=5e-5)\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    # Train the model on the current fold\n",
        "    train_model(model4, train_loader, val_loader, device, optimizer, scheduler, num_epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs_MNNRO8IpT"
      },
      "source": [
        "FOLD 0\n",
        "--------------------------------\n",
        "Epoch 1/3, Train Loss: 0.759006376476774\n",
        "Epoch 1/3, Validation Loss: 0.5358082576579338, Validation Accuracy: 0.7618243243243243\n",
        "Epoch 2/3, Train Loss: 0.5042485594240879\n",
        "Epoch 2/3, Validation Loss: 0.5358082576579338, Validation Accuracy: 0.7618243243243243\n",
        "Epoch 3/3, Train Loss: 0.5057545585348032\n",
        "Epoch 3/3, Validation Loss: 0.5358082576579338, Validation Accuracy: 0.7618243243243243\n",
        "\n",
        "FOLD 1\n",
        "--------------------------------\n",
        "Epoch 1/3, Train Loss: 0.7794085784017429\n",
        "Epoch 1/3, Validation Loss: 0.6341053394449724, Validation Accuracy: 0.7119932432432432\n",
        "Epoch 2/3, Train Loss: 0.5506814068291375\n",
        "Epoch 2/3, Validation Loss: 0.6341053394449724, Validation Accuracy: 0.7119932432432432\n",
        "Epoch 3/3, Train Loss: 0.5513873748201559\n",
        "Epoch 3/3, Validation Loss: 0.6341053394449724, Validation Accuracy: 0.7119932432432432\n",
        "\n",
        "FOLD 2\n",
        "--------------------------------\n",
        "Epoch 1/3, Train Loss: 0.7698235646551379\n",
        "Epoch 1/3, Validation Loss: 0.5582803849079837, Validation Accuracy: 0.7602739726027398\n",
        "Epoch 2/3, Train Loss: 0.5208293152750556\n",
        "Epoch 2/3, Validation Loss: 0.5582803849079837, Validation Accuracy: 0.7602739726027398\n",
        "Epoch 3/3, Train Loss: 0.5221011580155571\n",
        "Epoch 3/3, Validation Loss: 0.5582803849079837, Validation Accuracy: 0.7602739726027398\n",
        "\n",
        "FOLD 3\n",
        "--------------------------------\n",
        "Epoch 1/3, Train Loss: 0.6842181886015491\n",
        "Epoch 1/3, Validation Loss: 0.5422686889563522, Validation Accuracy: 0.761986301369863\n",
        "Epoch 2/3, Train Loss: 0.4475179272401862\n",
        "Epoch 2/3, Validation Loss: 0.5422686889563522, Validation Accuracy: 0.761986301369863\n",
        "Epoch 3/3, Train Loss: 0.44771099100747613\n",
        "Epoch 3/3, Validation Loss: 0.5422686889563522, Validation Accuracy: 0.761986301369863\n",
        "\n",
        "FOLD 4\n",
        "--------------------------------\n",
        "Epoch 1/3, Train Loss: 0.7059079891534795\n",
        "Epoch 1/3, Validation Loss: 0.5996909766164544, Validation Accuracy: 0.7217465753424658\n",
        "Epoch 2/3, Train Loss: 0.47543662216883065\n",
        "Epoch 2/3, Validation Loss: 0.5996909766164544, Validation Accuracy: 0.7217465753424658\n",
        "Epoch 3/3, Train Loss: 0.468779429335643\n",
        "Epoch 3/3, Validation Loss: 0.5996909766164544, Validation Accuracy: 0.7217465753424658"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUOkwUfD8JEs"
      },
      "outputs": [],
      "source": [
        "test_dataset = CustomDataset(\n",
        "    texts=test_df['Data'].to_numpy(),\n",
        "    labels=test_df['Labels'].tolist(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "# Create the DataLoader for the test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fmuqi7D8iEr"
      },
      "outputs": [],
      "source": [
        "model4.eval()\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model4(input_ids, attention_mask=attention_mask)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOkEalww8jBD"
      },
      "source": [
        "*Test* Accuracy: 0.80\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}